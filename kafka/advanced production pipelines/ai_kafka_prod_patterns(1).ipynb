{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "id": "56e53a1e",
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ðŸ­ AI + Kafka Production Patterns (Colab Demo)\n",
        "\n",
        "This notebook builds on the first demo and introduces **production-ready concepts** for AI + Kafka pipelines:\n",
        "\n",
        "1. **Schema Registry (contracts with Avro/JSON-Schema)**  \n",
        "2. **Dead-letter Queues (DLQ) for failed events**  \n",
        "3. **Consumer Groups & Scaling**  \n",
        "4. **Metrics & Monitoring Hooks**  \n",
        "\n",
        "Just like before, you can run in two modes:\n",
        "- `LOCAL_SIM` *(default)* â€” no Kafka account required (in-memory queue).\n",
        "- `KAFKA` â€” connect to Confluent Cloud with credentials.\n",
        "\n"
      ],
      "metadata": {
        "id": "56e53a1e"
      }
    },
    {
      "id": "8f23d01c",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f23d01c",
        "outputId": "5c45a3d5-93c5-4ad4-a883-400ef3e1f064"
      },
      "execution_count": 1,
      "source": [
        "\n",
        "# @title Install dependencies\n",
        "!pip -q install confluent-kafka==2.4.0                   fastavro==1.9.7                   prometheus-client==0.21.0                   faker==26.0.0                   pandas matplotlib\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "id": "91c37d44",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c37d44",
        "outputId": "d78a7e8e-b6e7-4b76-c09e-817dc3f0bc40"
      },
      "execution_count": 2,
      "source": [
        "\n",
        "# @title Configuration\n",
        "from dataclasses import dataclass\n",
        "\n",
        "MODE = \"LOCAL_SIM\"  # @param [\"LOCAL_SIM\", \"KAFKA\"]\n",
        "INPUT_TOPIC = \"events.raw\"\n",
        "OUTPUT_TOPIC = \"events.scored\"\n",
        "DLQ_TOPIC = \"events.dlq\"\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    mode: str\n",
        "    input_topic: str = INPUT_TOPIC\n",
        "    output_topic: str = OUTPUT_TOPIC\n",
        "    dlq_topic: str = DLQ_TOPIC\n",
        "    n_events: int = 50\n",
        "\n",
        "CFG = Config(mode=MODE)\n",
        "CFG\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Config(mode='LOCAL_SIM', input_topic='events.raw', output_topic='events.scored', dlq_topic='events.dlq', n_events=50)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "id": "0370c710",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0370c710",
        "outputId": "2ef7ad3c-53c8-4d72-dc54-1c590a60aa4a"
      },
      "execution_count": 3,
      "source": [
        "\n",
        "# @title Schema Registry (simulated with Avro)\n",
        "from fastavro import parse_schema, schemaless_writer, schemaless_reader\n",
        "import io\n",
        "\n",
        "# Define schema\n",
        "event_schema = {\n",
        "    \"type\": \"record\",\n",
        "    \"name\": \"Event\",\n",
        "    \"fields\": [\n",
        "        {\"name\": \"event_id\", \"type\": \"string\"},\n",
        "        {\"name\": \"user_id\", \"type\": \"string\"},\n",
        "        {\"name\": \"text\", \"type\": \"string\"},\n",
        "        {\"name\": \"lang\", \"type\": \"string\"},\n",
        "    ],\n",
        "}\n",
        "parsed_schema = parse_schema(event_schema)\n",
        "\n",
        "def serialize_avro(record: dict) -> bytes:\n",
        "    buf = io.BytesIO()\n",
        "    schemaless_writer(buf, parsed_schema, record)\n",
        "    return buf.getvalue()\n",
        "\n",
        "def deserialize_avro(payload: bytes) -> dict:\n",
        "    buf = io.BytesIO(payload)\n",
        "    return schemaless_reader(buf, parsed_schema)\n",
        "\n",
        "# Test it\n",
        "sample = {\"event_id\":\"123\",\"user_id\":\"42\",\"text\":\"hello world\",\"lang\":\"en\"}\n",
        "ser = serialize_avro(sample)\n",
        "print(\"Serialized bytes:\", ser[:20], \"...\")\n",
        "print(\"Deserialized:\", deserialize_avro(ser))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serialized bytes: b'\\x06123\\x0442\\x16hello world\\x04' ...\n",
            "Deserialized: {'event_id': '123', 'user_id': '42', 'text': 'hello world', 'lang': 'en'}\n"
          ]
        }
      ]
    },
    {
      "id": "bb2e99fb",
      "cell_type": "code",
      "metadata": {
        "id": "bb2e99fb"
      },
      "execution_count": 4,
      "source": [
        "\n",
        "# @title Local simulated topics (for LOCAL_SIM mode)\n",
        "import asyncio\n",
        "\n",
        "class LocalQueue:\n",
        "    def __init__(self):\n",
        "        self.q = asyncio.Queue()\n",
        "    async def produce(self, value: bytes):\n",
        "        await self.q.put(value)\n",
        "    async def consume(self, timeout=0.2):\n",
        "        try:\n",
        "            return await asyncio.wait_for(self.q.get(), timeout=timeout)\n",
        "        except asyncio.TimeoutError:\n",
        "            return None\n"
      ],
      "outputs": []
    },
    {
      "id": "1fd46d59",
      "cell_type": "code",
      "metadata": {
        "id": "1fd46d59"
      },
      "execution_count": 5,
      "source": [
        "\n",
        "# @title Producer with Avro encoding\n",
        "import uuid, random\n",
        "from faker import Faker\n",
        "fake = Faker()\n",
        "\n",
        "async def producer(cfg: Config, topic_q, n=10):\n",
        "    for i in range(n):\n",
        "        ev = {\n",
        "            \"event_id\": str(uuid.uuid4()),\n",
        "            \"user_id\": str(random.randint(1,100)),\n",
        "            \"text\": random.choice([\"great app\",\"bad service\",\"love it\",\"hate it\"]),\n",
        "            \"lang\": random.choice([\"en\",\"hi\"]),\n",
        "        }\n",
        "        await topic_q.produce(serialize_avro(ev))\n",
        "    print(\"[Producer] Done\")\n"
      ],
      "outputs": []
    },
    {
      "id": "9bc4a4f9",
      "cell_type": "code",
      "metadata": {
        "id": "9bc4a4f9"
      },
      "execution_count": 6,
      "source": [
        "\n",
        "# @title Worker with DLQ (if inference fails)\n",
        "import random\n",
        "\n",
        "async def worker(cfg: Config, in_q, out_q, dlq_q):\n",
        "    processed, failed = 0,0\n",
        "    while True:\n",
        "        msg = await in_q.consume()\n",
        "        if msg is None:\n",
        "            break\n",
        "        try:\n",
        "            ev = deserialize_avro(msg)\n",
        "            # Simulate failure\n",
        "            if random.random() < 0.2:\n",
        "                raise ValueError(\"Simulated inference error\")\n",
        "            # Enrich with fake sentiment\n",
        "            ev[\"sentiment\"] = random.choice([\"POSITIVE\",\"NEGATIVE\"])\n",
        "            await out_q.produce(serialize_avro(ev))\n",
        "            processed += 1\n",
        "        except Exception as e:\n",
        "            await dlq_q.produce(msg)\n",
        "            failed += 1\n",
        "    print(f\"[Worker] processed={processed}, failed={failed}\")\n"
      ],
      "outputs": []
    },
    {
      "id": "02041245",
      "cell_type": "code",
      "metadata": {
        "id": "02041245"
      },
      "execution_count": 7,
      "source": [
        "\n",
        "# @title Sink with Prometheus-style metrics\n",
        "from prometheus_client import Counter, CollectorRegistry, generate_latest\n",
        "\n",
        "registry = CollectorRegistry()\n",
        "c_processed = Counter(\"events_processed\",\"Events processed\",registry=registry)\n",
        "c_failed = Counter(\"events_failed\",\"Events failed\",registry=registry)\n",
        "\n",
        "async def sink(cfg: Config, out_q, dlq_q):\n",
        "    processed, failed = [],[]\n",
        "    while True:\n",
        "        msg = await out_q.consume()\n",
        "        if msg:\n",
        "            ev = deserialize_avro(msg)\n",
        "            processed.append(ev)\n",
        "            c_processed.inc()\n",
        "        msg2 = await dlq_q.consume()\n",
        "        if msg2:\n",
        "            ev2 = deserialize_avro(msg2)\n",
        "            failed.append(ev2)\n",
        "            c_failed.inc()\n",
        "        if not msg and not msg2:\n",
        "            break\n",
        "    print(\"[Sink] Final metrics:\")\n",
        "    print(generate_latest(registry).decode())\n",
        "    return processed, failed\n"
      ],
      "outputs": []
    },
    {
      "id": "b4934b7f",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4934b7f",
        "outputId": "1d066e22-8254-4165-f925-7cd428e885e2"
      },
      "execution_count": 8,
      "source": [
        "\n",
        "# @title Run the demo pipeline (LOCAL_SIM)\n",
        "async def main(cfg: Config):\n",
        "    in_q = LocalQueue()\n",
        "    out_q = LocalQueue()\n",
        "    dlq_q = LocalQueue()\n",
        "\n",
        "    await producer(cfg, in_q, cfg.n_events)\n",
        "    await worker(cfg, in_q, out_q, dlq_q)\n",
        "    results, failures = await sink(cfg, out_q, dlq_q)\n",
        "\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"Processed:\", df.head())\n",
        "    print(\"Failures:\", len(failures))\n",
        "\n",
        "import nest_asyncio, asyncio\n",
        "nest_asyncio.apply()\n",
        "await main(CFG)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Producer] Done\n",
            "[Worker] processed=37, failed=13\n",
            "[Sink] Final metrics:\n",
            "# HELP events_processed_total Events processed\n",
            "# TYPE events_processed_total counter\n",
            "events_processed_total 37.0\n",
            "# HELP events_processed_created Events processed\n",
            "# TYPE events_processed_created gauge\n",
            "events_processed_created 1.7565722780219715e+09\n",
            "# HELP events_failed_total Events failed\n",
            "# TYPE events_failed_total counter\n",
            "events_failed_total 13.0\n",
            "# HELP events_failed_created Events failed\n",
            "# TYPE events_failed_created gauge\n",
            "events_failed_created 1.756572278022097e+09\n",
            "\n",
            "Processed:                                event_id user_id         text lang\n",
            "0  d7ccf351-2549-4f61-9019-5bb7eeb92424      38  bad service   en\n",
            "1  78b5c6c3-b5ff-4c36-9a17-3347e0fdaeb7      46      love it   en\n",
            "2  4f7d19ee-994e-405e-a656-b08974c03853       6      love it   hi\n",
            "3  6ec7b4e4-dcce-4337-af74-c38bb173c4af      81  bad service   hi\n",
            "4  9ddcce45-bb93-472c-88a8-2a2f3faa8d73      30      hate it   en\n",
            "Failures: 13\n"
          ]
        }
      ]
    },
    {
      "id": "87c4ba08",
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ðŸ§° What You Learned\n",
        "\n",
        "- **Schema Registry**: We used Avro schemas to serialize/deserialize events â†’ prevents schema drift.  \n",
        "- **DLQ**: Failed events go to `events.dlq` so you donâ€™t lose data.  \n",
        "- **Consumer Groups**: (not simulated here, but in real Kafka multiple workers can share a topic â†’ scale out).  \n",
        "- **Metrics**: Exposed Prometheus counters for observability.  \n",
        "\n",
        "In production, youâ€™d also add:\n",
        "- **Retries before DLQ**\n",
        "- **Alerting on DLQ growth**\n",
        "- **Tracing with OpenTelemetry**\n",
        "- **Dashboards with Grafana**\n",
        "\n"
      ],
      "metadata": {
        "id": "87c4ba08"
      }
    }
  ]
}